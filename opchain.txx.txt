import pandas as pd
import numpy as np

# ------------------------------------------------------------
# Ensure tradelog has correct sorting before aggregation
# ------------------------------------------------------------
tradelog = tradelog.sort_values(["OptionID", "Date"]).copy()

# ------------------------------------------------------------
# Perform grouped aggregation (pandas summarise equivalent)
# ------------------------------------------------------------
allinfo = (
    tradelog
    .groupby("OptionID", as_index=False)
    .agg({
        "Date": "first",
        "Expiration": "first",
        "Price": ["first", "last"],
        "spread": ["first", "last"],
        "K": "first",
        "ImpliedVolatility": "first",
        "midprice": "first",
        "Delta": "first",
        "lixi": "last",
        "Vega": "first",
        "Gamma": "first",
        "cumHedgePnL": "last",
        "cumOpPnL": "last",
        "cumPnL": "last"
    })
)

# ------------------------------------------------------------
# Flatten MultiIndex column names (Date_first → Date, etc.)
# ------------------------------------------------------------
allinfo.columns = [
    "_".join(col).strip("_") for col in allinfo.columns.to_flat_index()
]

# ------------------------------------------------------------
# Rename columns to match R output exactly
# ------------------------------------------------------------
allinfo = allinfo.rename(columns={
    "Date_first": "Date",
    "Expiration_first": "Expiration",
    "Price_first": "startprice",
    "Price_last": "endprice",
    "spread_first": "startspread",
    "spread_last": "endspread",
    "K_first": "K",
    "ImpliedVolatility_first": "IV",
    "midprice_first": "startcost",
    "Delta_first": "Delta",
    "lixi_last": "final_lixi",
    "Vega_first": "Vega",
    "Gamma_first": "Gamma",
    "cumHedgePnL_last": "S_PnL",
    "cumOpPnL_last": "O_PnL",
    "cumPnL_last": "cumPnL"
})

# ------------------------------------------------------------
# Compute final PnL exactly as in R (placeholders kept)
# ------------------------------------------------------------
allinfo["PnL"] = (
    allinfo["cumPnL"]
    - 0 * 0.35 * allinfo["startspread"]   # same logic as R
    + 0 * allinfo["final_lixi"]
)

# ------------------------------------------------------------
# Optional: reorder columns for consistency with R output
# ------------------------------------------------------------
cols = [
    "OptionID", "Date", "Expiration",
    "startprice", "endprice",
    "startspread", "endspread",
    "K", "IV", "startcost",
    "Delta", "final_lixi", "Vega", "Gamma",
    "S_PnL", "O_PnL", "PnL"
]
allinfo = allinfo.reindex(columns=cols)

# ------------------------------------------------------------
# Done — inspect results
# ------------------------------------------------------------
print("✅ allinfo summary table:")
print(allinfo.head())

####




alldays = np.sort(optiondata["Date"].unique())
alldf = pd.DataFrame({"Date": pd.to_datetime(alldays), "flag": 1})

actiondates = (
    alldf
    .sort_values("Date")
    .groupby(alldf["Date"].dt.to_period("W"))["Date"]
    .max()
    .reset_index(drop=True)
)

# -------------------------------------------------------
# Pre-sort to make shift() operate correctly within OptionID
# -------------------------------------------------------
allinfo = allinfo.sort_values(["OptionID", "Date"]).copy()

# -------------------------------------------------------
# 1. Drop unwanted columns
# -------------------------------------------------------
drop_cols = [c for c in ["Volume", "OpenInterest"] if c in allinfo.columns]
allinfo = allinfo.drop(columns=drop_cols, errors="ignore")

# -------------------------------------------------------
# 2. Add K and Intrinsic
# -------------------------------------------------------
allinfo["K"] = allinfo["Strike"] / 1000.0

is_call = allinfo["CallPut"] == "C"
allinfo["Intrinsic"] = np.where(
    is_call,
    np.where(allinfo["Price"] > allinfo["K"], allinfo["Price"] - allinfo["K"], 0),
    np.where(allinfo["Price"] < allinfo["K"], allinfo["K"] - allinfo["Price"], 0)
)

# -------------------------------------------------------
# 3. midprice = Intrinsic if Expiration == Date else midprice
# -------------------------------------------------------
allinfo["midprice"] = np.where(
    allinfo["Expiration"] == allinfo["Date"],
    allinfo["Intrinsic"],
    allinfo["midprice"]
)

# -------------------------------------------------------
# 4. Fix Delta if abs(Delta) > 1
# -------------------------------------------------------
sign_from_cp = np.where(is_call, 1, -1)
has_intr = (allinfo["Intrinsic"] > 1e-9).astype(float)
mask = allinfo["Delta"].abs() > 1
allinfo.loc[mask, "Delta"] = sign_from_cp[mask] * has_intr[mask]

# -------------------------------------------------------
# 5. Compute lags within OptionID groups
# -------------------------------------------------------
allinfo["Price_lag"] = allinfo.groupby("OptionID")["Price"].shift(1)
allinfo["Delta_lag"] = allinfo.groupby("OptionID")["Delta"].shift(1)
allinfo["Fedrate_lag"] = allinfo.groupby("OptionID")["Fedrate"].shift(1)
allinfo["Midprice_lag"] = allinfo.groupby("OptionID")["midprice"].shift(1)

# -------------------------------------------------------
# 6. Derived fields (PnL and components)
# -------------------------------------------------------
allinfo["Price_diff"] = allinfo["Price_lag"] * allinfo["ret"]
allinfo["Op_PnL"] = allinfo["midprice"] - allinfo["Midprice_lag"]
allinfo["Delta_PnL"] = -1 * allinfo["Delta_lag"] * allinfo["Price_diff"]
allinfo["PnL"] = allinfo["Op_PnL"] + allinfo["Delta_PnL"]
allinfo["lixi"] = allinfo["Delta_lag"] * allinfo["Price_lag"] * allinfo["Fedrate_lag"]

# -------------------------------------------------------
# 7. Replace NAs with 0 for numeric columns
# -------------------------------------------------------
num_cols = allinfo.select_dtypes(include=[np.number]).columns
allinfo[num_cols] = allinfo[num_cols].fillna(0)

# -------------------------------------------------------
# 8. Cumulative sums within each OptionID (like cumsum)
# -------------------------------------------------------
allinfo["cumHedgePnL"] = allinfo.groupby("OptionID")["Delta_PnL"].cumsum()
allinfo["cumOpPnL"] = allinfo.groupby("OptionID")["Op_PnL"].cumsum()
allinfo["cumPnL"] = allinfo.groupby("OptionID")["PnL"].cumsum()
allinfo["cumlixi"] = allinfo.groupby("OptionID")["lixi"].cumsum()

# -------------------------------------------------------
# 9. Done
# -------------------------------------------------------
tradelog = allinfo.drop(columns=["Price_lag","Delta_lag","Fedrate_lag","Midprice_lag"])

print(tradelog.head())



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pathlib import Path

# =====================================================
# 0) Paths and config
# =====================================================


adPrices = pd.read_csv(data_path / "adPrices2ETFadj.csv")
stockprices = pd.read_csv(data_path / "stockreturnsETF.csv")
optiondata2 = pd.read_csv(data_path / "2005to2020ETFoptionsfewer.csv")

# =====================================================
# 1) Preprocess base data
# =====================================================

# Stock prices setup
stockprices = (
    stockprices
    .assign(SecurityID=lambda df: df["SecurityID"].astype(int))
    .sort_values(["Date", "SecurityID"])
)

# Option data filtering and column selection
optiondata2 = (
    optiondata2
    .assign(Date=lambda df: pd.to_datetime(df["Date"]),
            Expiration=lambda df: pd.to_datetime(df["Expiration"]))
    .query("Date >= '2009-01-01'")
    [["SecurityID","Date","Strike","Expiration","CallPut","BestBid","BestOffer",
      "Delta","Vega","Gamma","Theta","OptionID","ImpliedVolatility","Volume","OpenInterest"]]
)

# =====================================================
# 2) ID setup
# =====================================================
IDs = [106445,109820,107899,103823]
names = ["IWM","SPY","QQQ","DIA"]
delta1s = np.array([-0.05])
delta2s = np.array([-0.5])
nweeks = 1

# =====================================================
# 3) Loop over each delta pair and security
# =====================================================
for delta1, delta2 in zip(delta1s, delta2s):
    print(f"\n=== Running Δ-range {delta1} to {delta2} ===")
    for securityID, name in zip(IDs, names):
        print("Processing:", name, securityID)

        # --- Adinfo ---
        Adinfo = (
            adPrices
            .query("SecurityID == @securityID")
            [["Date","splitAdj","Adj"]]
            .drop_duplicates()
            .assign(Date=lambda df: pd.to_datetime(df["Date"]))
        )

        # --- Option subset ---
        optiondata = (
            optiondata2
            .query("SecurityID == @securityID and Date >= '2011-07-08'")
            .assign(time2mat=lambda df: (df["Expiration"] - df["Date"]).dt.days)
            .sort_values(["Date","SecurityID"])
        )

        alldates = optiondata["Date"].drop_duplicates().sort_values().values
        if len(alldates) == 0 or alldates[-1] < np.datetime64("2019-12-01"):
            print("  → Skipped (delisted or insufficient data)")
            continue

        # --- Read FedFund & RBsignal ---
        fedfund = (
            pd.read_csv(data_path / "fedfund.csv")
            .rename(columns={"Dates":"Date","Fedrate":"Fedrate"})
            .assign(Date=lambda df: pd.to_datetime(df["Date"], format="%m/%d/%Y"),
                    Fedrate=lambda df: df["Fedrate"]/100/252)
        )

        RBsignal = (
            pd.read_csv(data_path / "newsys3_5_VVIX.csv")
            .rename(columns={RBsignal.columns[0]: "Date"})
            .loc[:, ["Date","pos"]]
            .assign(Date=lambda df: pd.to_datetime(df["Date"]))
        )

        # =====================================================
        # 4) Create rebalance dates (weekly)
        # =====================================================
        alldays = np.sort(optiondata["Date"].unique())
        alldf = pd.DataFrame({"Date": pd.to_datetime(alldays), "flag": 1})
        alldf["week"] = alldf["Date"].dt.to_period("W").apply(lambda r: r.end_time)
        actiondates = alldf["week"].drop_duplicates().sort_values().reset_index(drop=True)

        results = []

        for i in range(1, len(actiondates)-nweeks):
            startdate = actiondates[i]
            enddate = actiondates[i+nweeks]
            time2exp = (enddate - startdate).days

            tempbuy = optiondata.query("Date == @startdate").copy()
            if tempbuy.empty:
                continue

            tempbuy = tempbuy.query("time2mat >= @time2exp")
            if tempbuy.empty:
                continue

            shorttest = tempbuy["time2mat"].min()
            tempbuy = tempbuy.query("time2mat == @shorttest")

            if delta1 < 0:
                tempbuy = tempbuy.query("CallPut == 'P'")
            else:
                tempbuy = tempbuy.query("CallPut == 'C'")

            # Spread ratio filter
            tempbuy = (
                tempbuy
                .assign(spreadratio=(tempbuy["BestOffer"]-tempbuy["BestBid"])/tempbuy["BestOffer"])
                .query("spreadratio < 0.6")
            )

            # Find ATM K1 / K2 by delta distance
            ATM_K1 = (
                tempbuy.assign(ddelta=(tempbuy["Delta"]-delta1).abs())
                .loc[lambda df: df["ddelta"]==df["ddelta"].min(),"Strike"]
                .values
            )
            ATM_K2 = (
                tempbuy.assign(ddelta=(tempbuy["Delta"]-delta2).abs())
                .loc[lambda df: df["ddelta"]==df["ddelta"].min(),"Strike"]
                .values
            )
            if len(ATM_K1)==0 or len(ATM_K2)==0:
                continue

            ATM_K1, ATM_K2 = float(ATM_K1[0]), float(ATM_K2[0])
            if ATM_K1 > ATM_K2:
                ATM_K1, ATM_K2 = ATM_K2, ATM_K1

            # Filter within the strike band
            Options = (
                tempbuy
                .query("Strike >= @ATM_K1 and Strike <= @ATM_K2 and abs(Delta) < 1")
                .sort_values("BestOffer")
                .drop_duplicates(subset=["Strike"], keep="first")
                .sort_values("Delta")
            )

            if Options.empty:
                continue

            OptionIDs = Options["OptionID"].unique()

            # Merge with stockprices, fedfund, and RBsignal
            merged = (
                optiondata.query("OptionID in @OptionIDs")
                .merge(stockprices, on=["Date","SecurityID"], how="inner")
                .merge(fedfund, on="Date", how="left")
                .merge(RBsignal, on="Date", how="left")
                .merge(Adinfo, on="Date", how="left")
                .assign(
                    midprice=lambda df: (df["BestBid"]+df["BestOffer"])/2,
                    spread=lambda df: (df["BestOffer"]-df["BestBid"]).abs(),
                    Fedrate=lambda df: df["Fedrate"].fillna(method="ffill").fillna(0),
                    pos=lambda df: df["pos"].fillna(-1),
                )
            )

            # Adjust prices for splits
            merged = (
                merged
                .sort_values(["OptionID","Date"])
                .groupby("OptionID", group_keys=False)
                .apply(lambda g: g.assign(
                    splitAdj=g["splitAdj"]/g["splitAdj"].iloc[0],
                    Price=g["Price"]*g["splitAdj"],
                    numcontract=g["splitAdj"]
                ))
            )

            # Compute PnL components
            tradelog = (
                merged
                .sort_values(["OptionID","Date"])
                .groupby("OptionID", group_keys=False)
                .apply(lambda g: g.assign(
                    K=g["Strike"]/1000,
                    Intrinsic=np.where(g["CallPut"]=="C",
                                       np.where(g["Price"]>g["Strike"], g["Price"]-g["Strike"], 0),
                                       np.where(g["Price"]<g["Strike"], g["Strike"]-g["Price"], 0)),
                    midprice=np.where(g["Expiration"]==g["Date"], 
                                      np.where(g["CallPut"]=="C",
                                               np.where(g["Price"]>g["Strike"], g["Price"]-g["Strike"], 0),
                                               np.where(g["Price"]<g["Strike"], g["Strike"]-g["Price"], 0)),
                                      g["midprice"])
                ))
            )

            tradelog = tradelog.fillna(0)
            results.append(tradelog)

        if not results:
            continue

        alltrades = pd.concat(results, ignore_index=True)

        # Summarize like allinfo
        allinfo = (
            alltrades
            .groupby("OptionID", as_index=False)
            .agg({
                "Date": "first",
                "Expiration": "first",
                "Price": ["first","last"],
                "spread": ["first","last"],
                "Strike": "first",
                "ImpliedVolatility": "first",
                "midprice": "first",
                "Delta": "first",
                "Vega": "first",
                "Gamma": "first"
            })
        )
        allinfo.columns = ["OptionID","Date","Expiration","startprice","endprice","startspread","endspread",
                           "K","IV","startcost","Delta","Vega","Gamma"]

        # Compute cumulative mean PnL like R
        finall = (
            allinfo
            .query("Date >= '2021-01-01'")
            .groupby("Date", as_index=False)
            .agg(AVGPnL=("Delta","mean"))  # dummy example; replace with actual PnL later
            .assign(AUM=lambda df: df["AVGPnL"].cumsum())
        )

        # Plot with seaborn
        sns.lineplot(x="Date", y="AUM", data=finall).set_title(name)
        plt.show()
